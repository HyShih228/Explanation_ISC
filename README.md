# Explanation_ISC
We provide an explanation of how a large language model (LLM) responds to instructions, with a particular focus on its intrinsic self-correction behavior. In the experimental section, we use the Zephyr-7B-SFT model on the RealToxicityPrompts dataset, and this repository contains the code used to conduct our experiments.

## Requirement

## Experiments
### get_hidden_states.py
### get_pca_results.py
### get_relation.py
### rounds_toxic.py
### utils.py
